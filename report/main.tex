\documentclass[12pt,twoside]{report}
\usepackage[left=2.5cm,right=2cm,top=2cm,bottom=2cm]{geometry}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Definitions for the title page
% Edit these to provide the correct information
% e.g. \newcommand{\reportauthor}{Timothy Kimber}

\newcommand{\reporttitle}{Constrained Reinforcement Learning for Process Optimisation}
\newcommand{\reportauthor}{Jaime Sabal Berm√∫dez}
\newcommand{\supervisor}{Dr. Calvin Tsay}
\newcommand{\degreetype}{Artificial Intelligence}


\newtheorem{lemma}[theorem]{Lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% load some definitions and default packages
\input{includes}

% load some macros
\input{notation}

\date{June 2022}

\begin{document}

% load title page
\input{titlepage}


% page numbering etc.
\pagenumbering{roman}
\clearpage{\pagestyle{empty}\cleardoublepage}
\setcounter{page}{1}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\fancyhead[RE,LO]{\sffamily {Table of Contents}}
\tableofcontents 

\pagenumbering{arabic}
\setcounter{page}{1}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Abstract}
Abstract should go here...

\chapter*{Acknowledgements}
Acknowledgements here...

\chapter{Introduction}

For all living organisms, the ability to learn by interacting with their environment is essential for survival. As humans, we learn how to interact with this world in a manner that allows us to maximise our time living in it, a primal instinct hard-wired into our consciousness through millions of years of evolution. However, existing in such a complex environment implies being subject to caveats which challenge and restrict the way in which we learn to adapt to it. For example, fish have evolved to only survive in aquatic settings and therefore act accordingly and instinctively to satisfy this constraint. In reality, there are usually a variety of constraints that an agent must consider to act safely. 

\smallskip

In the past few decades, and as computational power has vastly increased, machine learning has gained a lot of popularity for its capacity to imitate the way humans learn. This is done by solving problems numerically through the use of algorithms that mimic different aspects of cognitive reasoning. Following this definition, it makes sense to design a method that reflects the most basic way in which organisms make decisions - by interacting with their environment and adjusting their actions iteratively after making observations on the effect they had (i.e. after receiving a "reward" from the interaction). The sub-field of Reinforcement Learning (RL) \cite{Sutton1998} does this precisely, providing a framework for artificial agents to learn by interacting with an environment, analogously to how we as humans learn by interacting with our complex world. Reinforcement learning algorithms have proven to be very effective at solving tasks in areas such as gaming, where superhuman performance has been achieved in incredibly complex games like Go \cite{SilverHuangEtAl16nature}, or robotics and control, where often problems are too complex to be solved analytically.

\smallskip

Traditional RL involves an agent making decisions while continuously interacting with an environment freely, without being subject to any constraints. This means that there is no implicit cost associated with unlimited exploration of the environment and/or of specific states within it. Recent efforts have been made in the area of constrained RL, which attempts to address this limitation of traditional RL, to optimise the way in which an agent explores its environment and learns how to act whilst being subject to a set of constraints. Advancements in this area could lead the way to wide-spread use of reinforcement learning for industry applications. Currently, businesses often rely on poorly designed internal processes that lead to material and time wastes which could be easily avoided through the use of constrained reinforcement learning. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Theoretical Background}

\section{Reinforcement Learning}

Reinforcement learning defines a framework for the interaction between an \textit{agent} and an \textit{environment} in terms of the possible configurations of the environment as observed by the agent (i.e., the set of \textit{states} $\mathcal{S}$), the ways in which the agent can transition between states (i.e., the set of \textit{actions}  $\mathcal{A}$), and the benefit that being in state $S_t \in \mathcal{S}$ at time-step $t$ has towards achieving the agents' goal (i.e., the \textit{reward} $R_t \in \mathcal{R} \subset \mathbb{R}$). Moreover, the agent transitions between states according to the state transition probability function $\mathcal{P}$, which captures the dynamics of the environment. Its goal is then to learn a \textit{policy} $\pi$ that maps each state $s$ and action $a$ to the probability $\pi_t\!\left(A_t = a \! \mid \! S_t = s\right)$ of taking action $A_t$ in state $S_t$, and which maximises the \textit{total cumulative discounted reward} $G_t$ received over the future for all time-steps $t \in T$. This can be expressed as: 

\begin{equation}
    G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
\end{equation}

\noindent where $R_t$ is the reward received after taking action $A_t$ in state $S_t$ and $\gamma \in \left[0,1\right]$ is the \textit{discount rate}, which allows the infinite sum to have a finite value as long as the reward sequence $\lbrace R_k \rbrace$ is bounded. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--------------MDP's-------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Markov Decision Processes}

Reinforcement learning problems are usually said to have the \textit{Markov property} and therefore are assumed to be \textit{Markov decision processes} (MDP's), meaning that the probability of transitioning to a new state $s'$, given the current state $s$ and action $a$, is conditionally independent of all previous states and actions and as such:

\begin{equation}
    \mathbb{P}\left[S_{t+1} = s' \! \mid \! S_t, A_t \right] =  \mathbb{P}\left[S_{t+1} = s' \! \mid \! S_0, A_0, ..., S_{t-1}, A_{t-1}, S_t, A_t \right]
\end{equation}

This property implies that the current state contains all the relevant information about the past needed to predict future rewards and select actions, which is useful since it reduces problems to functions of the current state only. Formally, an MDP is defined by $\left(\mathcal{S}, \mathcal{A}, \mathcal{R}, \mathcal{P}, \gamma\right)$, where $\mathcal{P}$ is the transition probability function, $\gamma$ is the discount rate, and $\mathcal{S}$, $\mathcal{A}$, and $\mathcal{R}$ are the state, action, and reward spaces, respectively.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--------VALUE FUNCTIONS---------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Value Functions}

Now that we have defined a framework for reinforcement learning problems, it is convenient to define a metric that reflects the potential of a specific state with respect to the goal of maximising future rewards. Usually, the objective in reinforcement learning problems (i.e., MDP's) is to learn the optimal \textit{value function} $V^*(s)$, expressed as: 

\begin{equation}
\begin{split}
        V^*(s) & = \mathbb{E}_{\pi^*}\left[G_t \! \mid \! S_t = s \right] \\
        & = \max_{\pi} V_\pi(s)
\end{split}
\end{equation}

\noindent which gives a numerical "score" to each state $s$ from the expected value of $G_t$ given $s$ under the optimal policy $\pi^*$. Learning the optimal value function directly leads us to find the optimal policy, since for two different policies $\pi$ and $\pi'$ we have that $\pi \geq \pi' \Longleftrightarrow V_{\pi}(s) \geq V_{\pi'}(s)$, $\forall s$ is always true \cite{Sutton1998}, where the subscripts on the value functions indicate the policy being followed. Moreover, it is often useful to additionally define the \textit{state-action value function} (2.4) and its optimal equivalent (2.5) as:

\begin{equation}
    Q_\pi(s,a) = \mathbb{E}_{\pi}\left[G_t \! \mid \! S_t = s, A_t = a \right] 
\end{equation}

\begin{equation}
     Q^*(s,a) = \max_{\pi} Q_\pi(s,a)
\end{equation}

\noindent since we express policies as probabilities of choosing an action $a$ from a state $s$. The value function can then be expressed in terms of $Q_\pi(s,a)$ as:

\begin{equation}
    V_\pi(s) = \sum_{a \in \mathcal{A}} Q_\pi(s,a)\pi(s,a)
\end{equation}

Consequently, the optimal policy $\pi^*$ is obtained through the maximisation of the value function: 

\begin{equation}
    \pi^* = \argmax_\pi V_\pi(s) = \argmax_\pi Q_\pi(s,a)
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--------BELLMANS EQUATIONS------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Bellman's Equations}

Combining (2.1) and (2.3), we can derive the following recursive relationship in value functions: 

\begin{equation}
\begin{split}
       V_\pi(s) &  = \mathbb{E}_{\pi}\left[R_{t+1} + \gamma \sum_{k=0}^\infty \gamma^k R_{t+k+2} \! \mid \! S_t = s \right] \\
       & = \mathbb{E}_{a \sim \pi, s' \sim \mathcal{P}}\left[r(s,a) + \gamma V_\pi(s') \right]
\end{split}
\end{equation}

\noindent where the action $a$ is sampled from the policy $\pi$, and the subsequent state $s'$ from the transition probability function $\mathcal{P}$. Equivalently for the state-action value function: 

\begin{equation}
    Q_\pi(s,a) = \mathbb{E}_{s' \sim \mathcal{P}} \left[r(s,a) + \gamma  \mathbb{E}_{a' \sim \pi} \left[Q_\pi(s',a')\right] \right]
\end{equation}

\noindent which together with (2.8) yield \textit{Bellmans equation's}, and provide an update rule to iteratively improve approximations of value functions through the use of \textit{dynamic programming} \cite{bellman_1958} (see Section 2.1.2), which ensures convergence to the optimal value function. In this manner, a complex problem that is recursive in nature (often the case for problems that span over time) can be broken down into simpler sub-problems and solved optimally and efficiently. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--------TRADITIONAL METHODS-----
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Traditional Methods}

Traditional or \textit{tabular} RL refers to problems where state and action spaces are small enough that optimal value functions can be approximated using look-up tables/arrays by using the Bellman equations. RL algorithms can be divided into model-based and model-free methods. The former require knowledge of the underlying environment dynamics (i.e. state-transition probabilities) through the use of a \textit{model}. This model can be exact (e.g. if there are a clear set of rules that govern these dynamics like in a game of chess) or an approximation learned and provided to the agent by an external source (e.g. a neural network). On the other hand, model-free methods learn from sampled experience, with an approximation of these dynamics learned intrinsically in finding the optimal value function. This is a much more practical approach for real scenarios where this information isn't readily available. 

\subsubsection{Dynamic Programming}

Dynamic programming (DP) refers to a collection of algorithms that are conceptually very important in understanding any RL problem, but that are limited in that they assume a perfect model of the environment dynamics (i.e. model-based) and are computationally very expensive. Exact solutions are found for the system of equations that govern a MDP through \textit{bootstrapping} (i.e. using an estimate of a value to get closer to its true value), without the agent really "learning" anything. 

\smallskip

\textit{Value iteration} is an algorithm whereby the optimal value function is iteratively found by taking the maximum value over all actions at every step using Bellman's equation and subsequently updating the policy at the end to its optimal form. Alternatively, \textit{policy iteration} involves evaluating an initially random policy by finding its value function and improving it by taking the best action (as per the found value function) for all states, repeating this process until a stability condition is satisfied. 

\subsubsection{Monte-Carlo Control}

Monte-Carlo (MC) control \cite{Sutton1998} is a model-free RL algorithm that addresses the limitation of dynamic programming of requiring access to the state-transition probabilities. It relies on sampling to calculate the value of a state or state-action pair from the average empirical return seen for it across all samples. The update rule on $Q(s,a)$ is thus: 

\begin{equation}
    Q(s,a) \longleftarrow Q(s,a) + \alpha \left[G_t - Q(s,a)\right]
\end{equation}

\noindent where $\alpha$ is the learning rate and $G_t$ the average total discounted reward at time $t$ staring at state $s$. 

By the law of large numbers, the state-action value function converges to its optimal value in the limit of infinite samples and thus the optimal policy can be found. However, sampling presents a lot of variance so this algorithm could take a long time to converge.  

\subsubsection{Temporal-Difference Learning}

Temporal-Difference learning (TD-learning) is a model-free idea that also uses the concept of bootstrapping to update the values of the states. Similarly to Monte-Carlo control the TD target is sampled, but it also includes an estimate of $Q(S_{t+1},A_{t+1})$ as in dynamic programming. \textit{SARSA} \cite{sarsa} is an on-policy (i.e. uses a behaviour policy to estimate the next state-action value) TD control algorithm with the update step: 

\begin{equation}
    Q(S_t,A_t) \longleftarrow Q(S_t,A_t) + \alpha \left[R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)\right]
\end{equation}

\noindent where $\gamma$ is the discount factor. Q-learning \cite{q-learning} is the off-policy alternative to SARSA, approximating the optimal state-action value function independently from the policy being followed. Rather, it uses $Q(S_{t+1},A_{t+1}) = \max_a Q(S_{t+1},a)$ for the TD target: 

\begin{equation}
    Q(S_t,A_t) \longleftarrow Q(S_t,A_t) + \alpha \left[R_{t+1} + \gamma \max_a Q(S_{t+1},a) - Q(S_t,A_t)\right]
\end{equation}

The presence of bootstrapping comes with the caveat of adding bias to the update targets, since these will be dependent on the starting values of the states. This can potentially lead to instability in training, with value over-estimations often occurring in the case of Q-learning due to the $\max$ operator. It is also worth mentioning the trade-off that there exists between \textit{exploration} and \textit{exploitation} in RL problems. That is, we want to explore as much of the state space as possible to allow all states to converge to their optimal values before always choosing \textit{greedy} actions (i.e. those with the highest value). A way of managing this trade-off is using $\epsilon$-greedy policies that select greedy actions with probability $1- \epsilon$ and random actions with probability $\epsilon$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%------------DEEP RL-------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Deep Reinforcement Learning}

Traditional reinforcement learning algorithms suffer from the \textit{curse of dimensionality} \cite{bellman-dp}. This phenomenon describes the problem of obtaining reliable results when dealing with a high-dimensional state space, arising from the fact that the amount of data needed to achieve a result increases exponentially with dimensionality. Using a non-linear function approximator such as a deep neural network with parameters $\theta$, we are able to estimate the optimal state-action value function (or policy directly) as $Q(s,a; \theta) \approx Q^*(s,a)$ for very complex and large state and action spaces. 

\subsubsection{Deep Q-Networks}

The concept of a Deep Q-Network (DQN) \cite{deep-rl} was introduced by DeepMind in a successful attempt to have an artificial agent achieve better performance than humans on seven Atari 2600 games. It consists of training a deep neural network with parameters $\theta$ to estimate the optimal state-action value function by minimising the mean-squared error (MSE) with a TD target. For a given optimisation step $i$, this loss can be expressed as: 

\begin{equation}
    L_i(\theta_i) = \mathbb{E}_{s,a,r,s' \sim \rho} \left[ \left(y_i - Q(s,a; \theta_i) \right)^2\right]
\end{equation}

\noindent where $y_i = r + \gamma \max_{a'} Q(s',a'; \hat{\theta})$ is the TD target and $\rho$ is the behaviour distribution from which new states are sampled. The parameters $\theta$ are then updated accordingly using stochastic gradient descent. It is important to note how $\max_{a'} Q(s',a')$ is calculated using a different network with parameters $\hat{\theta}$, which are a copy of $\theta$ from a number of  This is done to stabilise training, since $Q(s,a)$ and $Q(s', a'), a' \in \mathcal{A}$ would be largely correlated otherwise. Moreover, a novel concept introduced in \cite{experience-replay} and used in DQN's is that of \textit{experience replay}. This consists of randomly sampling previous transitions from a buffer of saved examples, preventing learned information from being forgotten and smoothing the training distribution by reducing the correlation between samples in the same training step. In combination with using a target network, this adjustment dramatically improves the performance and stability of the DQN \cite{deep-q-networks}. 

\smallskip

A variation for the DQN is the Double Deep Q-Network (DDQN) \cite{DDQN}, which decouples the way in which actions are selected and evaluated by using the target and behaviour networks separately for these tasks. Doing this diminishes the overestimation of values resulting from the $\max$ operator being used for the TD target. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%------------POLICY GRADIENTS-------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Policy Gradients}

Policy gradients are useful for problems dealing with continuous action spaces, which are commonly found in reality. In policy gradient methods, we learn a parameterised function for the policy directly, written as $\pi_\theta(a \! \mid \! s)$, in terms of $\theta$. To do this, a neural network is optimised to maximise the reward function: 

\begin{equation}
\begin{split}
    J(\theta) & = \sum_{s \in \mathcal{S}} d^\pi (s) V^\pi (s) \\
    & = \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta (a \! \mid \! s) Q^\pi (s,a)
\end{split}
\end{equation}

\noindent where $d^\pi(s)$ is the stationary probability  of $s$ (i.e. the probability of $s$ having started from an initial state $s_0$ under $\pi_\theta$). Since $J(\theta)$ is a function of $\pi_\theta$, we can use gradient ascent to perform steps on $\theta$ in the direction of $\nabla_\theta J(\theta)$. However, this gradient depends on the unknown effect of the policy parameters on the state distribution $d^\pi(s)$, which is a function of the environment. The \textit{policy gradient theorem} \cite{policy-gradients} simplifies this problem by giving an analytic expression for the gradient excluding $\nabla_\theta d^\pi(s)$: 

\begin{equation}
    \nabla_\theta  J(\theta) = \mathbb{E}_\pi \left[Q^\pi(s,a) \nabla_\theta \ln \pi_\theta(a \! \mid \! s) \right]
\end{equation}

\noindent which results in an update rule that presents no bias but a large variance, the latter commonly alleviated through the use of the advantage $A(s,a) = Q(s,a) - V(s)$ in place of $Q(s,a)$. One of the first policy gradient methods was REINFORCE  \cite{Sutton1998}, an on-policy algorithm which relies on finding the episodic expected value of the return through Monte-Carlo sampling to optimise $\theta$ - exploiting the fact that the expected value of the sampled gradient is equal to the true gradient. 

\smallskip

\subsubsection{Trust Region Policy Optimisation}

As has been discussed, one of the biggest drawbacks of policy gradient methods is the presence of large variance and thus instability in training; Trust Region Policy Optimisation (TRPO) \cite{trpo} addresses this issue by avoiding updates of the policy parameters that lead to very large changes in the policy. Specifically, it does this by enforcing the KL-divergence between old and new policies to be smaller than a threshold $\delta$, called the \textit{trust region constraint}. In this manner, TRPO is able to guarantee monotonic improvement over policy iteration.

\textcolor{red}{Need to say more about TRPO since it is essential to my project.}

\subsubsection{Other Methods}

Actor-critic methods \cite{actor-critic-algorithms} attempt to learn both the value function and the policy, using the value function to address large variances in training (usually in an on-policy fashion). Specifically, at each step in training, the critic performs gradient ascent on the value function parameters $\phi$, and the actor subsequently updates the policy parameters $\theta$ in the same direction as the critics gradient. The Soft Actor-Critic (SAC) \cite{SAC, PG-algorithms} algorithm provides an off-policy rendition of this idea with the objective of improving sampling efficiency by re-using past trajectories. It also offers a slight variation in that the policy is optimised to maximise both the expected return and the entropy $\mathcal{H}$ to promote exploration. This objective can be expressed as: 

\begin{equation}
    J(\theta) = \sum_{t=0}^{T} \mathbb{E}_{(s_t, a_t) \sim \rho_{\pi_\theta}} \left[r(s,a) + \alpha \mathcal{H}\left(\pi_\theta(\cdot \! \mid \! s_t\right)\right]
\end{equation}

\noindent where $\rho_{\pi_\theta}$ is the state-action marginal distribution induced by the policy $\pi_\theta$. It is worth mentioning that we want the policy to be tractable, so in practice we restrict the policy to a set $\Pi$ (e.g. Gaussian distributions). We then update it through the minimisation of the Kullback-Leibler (KL) divergence between $\pi_\theta (\cdot \! \mid \! s)$ and the normalised exponential of the updated actor network $Q_\phi(s,a)$. Another off-policy algorithm that uses the actor-critic framework is Deep Deterministic Policy Gradients (DDPG) \cite{DDPG}. Its objective is to learn a deterministic policy for continuous action spaces by learning both a Q-function $Q_\phi(s,a)$ (through the framework of a DQN) and a policy $\pi_\theta \! \left(a \! \mid \! s\right)$ that maximises $Q_\phi(s,a)$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%------------DISTRIBUTIONAL RL--------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Distributional Reinforcement Learning}

Traditionally, and when not subject to constraints, the objective of RL agents is to maximize the value function across the state space to obtain the optimal policy. In \textit{distributional reinforcement learning} \cite{distributional-perspective} (DRL), this objective changes to learning the distribution of values for all state-action pairs. This approach provides a range of auxiliary statistics (e.g. the variance in the return intrinsic to the randomness of an MDP) that aid in providing a more natural framework for inductive bias

\smallskip

DRL focuses on optimizing the distribution of the random return $Z^\pi \! \left(s,a\right)$ having started in state $s$ and taken action $a$ under policy $\pi$, whose expectation value is the state-action value function $Q^\pi \! \left(s,a\right)$. It can also be expressed as a recursive equation using the \textit{distributional Bellman equation}: 

\begin{equation}
    \mathcal{T}^\pi Z(s,a) \stackrel{D}{\coloneqq} R(s,a) + \gamma Z(s',a')
\end{equation}

\noindent where $\mathcal{T}^\pi : \mathcal{Z} \longrightarrow \mathcal{Z}$, with $\mathcal{Z}$ being the space of action-value distributions, is the distributional Bellman operator, and $s' \sim P(\cdot \! \mid \! s,a), a' \sim \pi(\cdot \! \mid \! a')$. This equation is characterised in terms of three independent random variables: the reward $R(s,a)$, the next state-action pair $(s',a')$, and its return $Z(s',a')$. 

\subsubsection{Quantile Networks}

It can be shown that the distributional Bellman operator $\mathcal{T}^\pi$ is a $\gamma$-contraction in the Wasserstein distance $\mathcal{W}$ with a unique fixed point at $Z^\pi$  \cite{distributional-perspective}. This metric is useful because it poses an update rule analogous to the target in TD methods that converges to $Z^\pi$ when $\mathcal{W}$ is minimised. However, this can't be done generally through stochastic gradient descent because the minimum expected sample Wasserstein loss is different from its true value \cite{quantile-regression}, which complicates the task at hand. 

\smallskip

Quantile Regression DQN (QRDQN) \cite{quantile-regression} aims to optimise the return distribution through its parameterization into a discrete set of $N$ Dirac delta distributions whose locations in $R \in \mathbb{R}$ are adjusted using quantile regression to be able to minimise $\mathcal{W}$ easily. The Implicit Quantile Network (IQN) \cite{quantile-networks} builds upon this idea and reparameterizes a base distribution $\tau \sim U(\left[0,1\right])$ (the implicit return distribution and prior) to fit the quantile values of $Z^\pi$; in this manner, $Z^\pi$ can be learned without imposing any assumptions on its parameterization. Moreover, in \cite{quantile-networks} it was found that increasing the number of quantiles led to faster learning, corroborating that the added benefit of DRL algorithms lies in their effect as auxiliary loss functions, stabilising training and serving as regularizers. 

\smallskip

In practice, we approximate the return distribution through a neural network $y_i(\theta,s,a)$ parameterized by $\theta$ and with $N$ outputs indicating the number of quantiles $\tau$ \cite{risk-uncertainty}. We then attempt to maximise the likelihood: 

\begin{equation}
    P(D \! \mid \! \theta) = \prod_{j=1}^{K}\prod_{i=1}^{N} f_{\tau_i}(z_j - y_i(\theta,s,a))
\end{equation}

\noindent where $f_{\tau_i}(u) = \frac{\tau(1-\tau)}{\sigma_D}\exp(-\frac{\rho_\tau(u)}{\sigma_D})$. Here $\sigma_D$ is a characteristic length scale and $\rho_\tau (u) = u \times (\tau_i - \mathds{1}_{u < 0})$. We can finally approximate the posterior distribution $P(\theta \! \mid \! D)$ from the likelihood and prior through Bayesian inference by using Markov Chain Monte-Carlo (MCMC) methods \cite{MCMC}. 

\subsubsection{Estimating Risk and Uncertainty}

Any model-free RL agent faces two types of uncertainty: that arising from having limited knowledge about the model governing the dynamics of the environment (\textit{epistemic uncertainty}), and that stemming from the stochasticity present in any MDP (\textit{aleatoric uncertainty}). However, these are in practice difficult to quantify individually because the variance of the quantiles in $Z$ for out of distribution data can be affected by them both.

\smallskip

In \cite{risk-uncertainty} they propose a way of disentangling these two types of uncertainty and using them individually to tune the risk-adversity of the agent (using aleatoric uncertainty) and optimise exploration (through epistemic uncertainty); the former purely in the sense of maximising rewards, without considering constraints in the MDP. Following the framework of quantile networks described previously, aleatoric uncertainty $\sigma_\text{aleatoric}^2$ can be defined as the variance of the expected value of the quantiles taken from $P(\theta \! \mid \! D)$ over $\theta$: 

\begin{equation}
    \sigma_\text{aleatoric}^2 = \text{var}_{i \sim \mathcal{U}\{1,N\}} [\mathbb{E}_{\theta \sim P(\theta \mid D)} y_i (\theta,s,a) ]
\end{equation}

\noindent where $\mathcal{U}\{1,N\}$ is the uniform distribution over $\{1,N\}$. 

Moreover, the epistemic uncertainty can then be thought of as expected value of the variance of the quantiles over $\theta$: 

\begin{equation}
    \sigma_\text{epistemic}^2 = \mathbb{E}_{i \sim \mathcal{U}\{1,N\}} [\text{var}_{\theta \sim P(\theta \mid D)} (y_i (\theta,s,a)) ]
\end{equation}

In the limit of infinite data, when the posterior is concentrated around a single value, all of the variance in the quantile values is aleatoric, while in the absence of data all of the uncertainty is epistemic. In practice, we estimate these error sources by using two auxiliary neural networks that draw samples from $P(\theta \! \mid \! D)$ using MAP sampling \cite{map-sampling}, and use them to perform information-directed exploration by, for example, penalising actions with a high aleatoric uncertainty or foment exploration around states with a large epistemic uncertainty \cite{risk-uncertainty}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%------------CONSTRAINED RL-----------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Constrained Reinforcement Learning}

As described in the introduction of this investigation, it is very difficult to think about a reinforcement learning scenario in nature without considering constraints. It is also very important to consider situations where a system coexists physically with humans; in this case, it should satisfy the set of constraints that ensures maximal safety for the humans around it. 

It is common for constrained reinforcement learning (CRL) algorithms to introduce safety constraints in the context of a constrained MDP (CMDP), which additionally maps transition tuples to costs $c(s,a) \in \mathbb{R}$ in a way analogous to rewards, and adds a safety threshold $d$ to the formulation of a regular MDP \cite{conservative-distributional-rl}. The objective function of the agent becomes to maximise rewards such that the safety threshold isn't surpassed by the cost function, expressed as: 

\begin{equation}
\begin{split}
    & \max_\pi \hspace{2}  \mathbb{E}_{(s,a) \sim \rho_\pi} [\sum_t \gamma^t r(s, a)], \\ 
    & \textrm{s.t.} \hspace{5} \mathbb{E}_{(s,a) \sim \rho_\pi} [\sum_t \gamma^t c(s, a)] \le d. 
\end{split}
\end{equation}

The constraint aspect of CMDP's are then usually solved using the Lagrange multiplier method, transforming Eq. (2.21) into the unconstrained dual problem: 

\begin{equation}
    \min_{\lambda \geq 0} \max_{\theta} L(\theta, \lambda) = J_{R}^{\pi_\theta} - \lambda(J_{C}^{\pi_\theta} - d)
\end{equation}

\noindent where $J_{R}^{\pi_\theta} = \mathbb{E}_{(s,a) \sim \rho_\pi} [\sum_t \gamma^t r(s, a)]$ and $J_{C}^{\pi_\theta} = \mathbb{E}_{(s,a) \sim \rho_\pi} [\sum_t \gamma^t c(s, a)]$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%-------------CPO-----------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Constrained Policy Optimization}

A way of applying constraints to a RL problem is by mapping transition tuples to costs in a way analogous to what is done with rewards in regular MDP's. Constrained Policy Optimisation (CPO) \cite{cpo} uses the framework of a CMDP, whereby policies are restricted to a set $\pi \in \Pi_\theta$ that must satisfy the following relations: 

\begin{itemize}
    \item the cost "returns" $J_{C_i}(\pi)$ (that include the constraints we want to impose on the agent) must be smaller than a cost limit $d_i, \forall i$ for a given state $s$: 
    
    \begin{equation}
         J_{C_i}(\pi_k) + \frac{1}{1 - \gamma} \mathbb{E}_{s \sim d^{\pi_k}, a \sim \pi} [A_{C_i}^{\pi_k}(s,a)] \le d_i \hspace{10} \forall i 
    \end{equation}
    
    \noindent where $A_{C_i}^{\pi_k}$ denotes the cost advantage for constraint $i$ of being in state $s$ and taking action $a$ under policy $\pi_k$, and $d^{\pi_k}$ is the discounted future state distribution $d^\pi (s) = (1 - \gamma) \sum_{t=0}^\infty \gamma^t P(s_t=s \! \mid \! \pi)$. 
    
    \item a \textit{trust region} inspired by trust region optimization methods (see Section 2.1.3) that limits the expected value of the KL divergence between the old and new policies at state $s$ to a step size $\delta$: 
    
    \begin{equation}
        \mathbb{E}_{s \sim \pi_k} [\overline{D}_{KL} (\pi \mid \mid \pi_k)[s]] \le \delta
    \end{equation}
    
\end{itemize}

\noindent By doing this, we are able to perform a policy update that guarantees monotonic performance improvements and constraint satisfaction. The policy update can be expressed as: 

\begin{equation}
    \pi_{k+1} = \arg \max_{\pi \in \Pi_\theta} \mathbb{E}_{s \sim d^{\pi_k}, a \sim \pi} [A^{\pi_k}(s,a)] 
\end{equation}

\noindent which is very costly computationally for solutions requiring high-dimensional parameter spaces like neural networks. For small step sizes $\delta$, however, approximating the returns and costs becomes computationally feasible by linearizing around $\pi_k$ and performing policy updates that resemble primal-dual methods \cite{cpo}. As a final note, it is is worth mentioning that this algorithm presents a limitation in that intermediary policies are not required to satisfy the constraints, but rather only the converged policy is. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%-------------SAUTE-----------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{State-Augmented Markov Decision Processes}

Another way of applying constraints to a RL problem is by augmenting the state-space and reshaping the objective slightly by adding a budget for the number of constraint violations that can occur. Safety Augmented (SAUTE) Markov Decision Processes \cite{saute} do this effectively in a "plug-and-play" approach that can be applied to most RL algorithm seamlessly. The state-space is augmented through the variable $z_t \in \mathcal{Z}$ defined by the Markovian relation: 

\begin{equation}
    z_{t+1} = (z_t - l(s_t, a_t))/\gamma_l
\end{equation}

\noindent where $z_0 = d$, $l(s_t,a_t) \in [0,+\infty]$ is the safety cost associated with state-action pair $(s_t,a_t)$, and $\gamma_l \in [0,1]$ is the safety discount factor. With this in mind, we define a SAUTE MDP in a slightly different way than a CMDP (which is represented by $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \tilde{c}, \gamma_c)$) through the tuple $(\tilde{\mathcal{S}}, \mathcal{A}, \tilde{\mathcal{P}}, \tilde{c}_n, \gamma_c, \gamma_l)$, where $\tilde{\mathcal{S}}: \mathcal{S} \times \mathcal{Z}$; $\tilde{\mathcal{P}}: \tilde{\mathcal{S}} \times \mathcal{A} \times \tilde{\mathcal{S}} \longrightarrow [0,1]$, $\gamma_c$ is the cost discount factor and $\tilde{c}_n (s_t, z_t, a_t)$ is the cost expressed as: 

\begin{equation}
    \tilde{c}_n (s_t, z_t, a_t) = 
    \begin{cases}
        c(s_t, a_t) & z_t \geq 0, \\
        n & z_t < 0.
    \end{cases}
\end{equation}

\noindent where $n$ is a constant. We thus express the objective function in terms of a minimisation of costs rather than a maximisation of rewards: 

\begin{equation}
    \min_\pi \mathbb{E} \sum_{t=0}^{\infty} \gamma_c^t \tilde{c}_n (s_t, z_t, a_t)
\end{equation}

\noindent and it can be proved that there exists a value function that converges monotonically to the optimal value function \cite{saute}. Finally, it is worth mentioning that a limitation of this approach is that the state-space increases with the number of constraints, which could lead to scalability issues. In this regard, Lagrangian-based methods can be more sample efficient. 

\chapter{Inventory Management Problem}

\section{Formulation}

\chapter{Developing the Framework}

Use \verb|this| style to name modules!

\chapter{Distributional Constrained Policy \\ Optimization}

\section{Distributional Value Function and Safety Baseline}

\section{Cost Reshaping}

Let $\rho_v = \beta \mathbb{P}  (J_{C} > d)\right)$ and  $\rho_s =  - \beta \mathbb{P}  (J_{C} < d)\right)$ denote the refactoring parameters when the constraints are being violated and satisfied, respectively. A general equation for the refactoring parameter  $\rho$ can then be expressed as: 

\begin{equation}
\begin{gathered}
\rho = \left\{\begin{array}{ll}
	 \rho_v, & J_{C} > d \\
	 \rho_s, & J_{C} < d \\
	 	\end{array}
	 \right. \\
	 
	 & = \left\{\begin{array}{ll}
		\beta \mathbb{P} \left( J_{C} > d )\right), & J_{C} > d \\
		 - \beta \mathbb{P}  \left( J_{C} < d \right)  & J_{C} < d  \\
		 	\end{array}
		\right.
\end{gathered}
\end{equation}

\noindent An expression for the refactored costs and target is then: 

\begin{equation}
\tilde{J}_{C} = J_C \left( 1 + \textrm{clip} \left( \rho, -\epsilon, \infty \right) \right) \\
\end{equation}

\begin{equation}
\tilde{d} =  d  \left( 1 + \textrm{clip} \left( \rho, -\epsilon, \infty \right) \right) \\
\end{equation}

\begin{lemma}
Hey
\end{lemma}

\begin{proof}  
why this works:
\end{proof}

\begin{algorithm}[hbt!]
\caption{Distributional Constrained Policy Optimization}\label{alg:dcpo}
\KwInput{Initial policy $\pi_0 \in \Pi_\theta$, safety margin $\epsilon$, refactoring coefficient $\beta$}
 \For{$k=0,1,2,...$}{
  Sample a set of trajectories $\mathcal{D} = {\tau} \sim \pi_k = \pi\left(\theta_k\right)$\\
  Obtain reshaped constraint $\tilde{J}_{C}\left(\pi_k\right)$ and target $\tilde{d}$\\
  Form sample estimates $\hat{g}$, $\hat{b}$, $\hat{H}$, $\hat{c}$ using $\mathcal{D}$, $\tilde{J}_{C}\left(\pi_k\right)$ and $\tilde{d}$\\
  \eIf{approximate CPO is feasible}{
   Solve dual problem (Eq. ref) for $\lambda^\ast_k$, $\nu^\ast_k$\\
   Compute policy proposal $\theta^\ast$ with (Eq. ref)
   }{
   Compute recovery policy proposal $\theta^\ast$ with (Eq. ref)
  } 
  Obtain $\theta_{k+1}$ through backtracking linesearch to enforce satisfaction of sample estimates of constraints in (Eq. ref)
 }
\end{algorithm}

\section{From Progress Report...}
We define the objective of this investigation as the approximation of the optimal distributions of costs and returns as per the Bellman optimality equations to obtain the optimal policy in situations where constraint satisfaction is of high importance. By approximating distributions of both costs and returns, we are able to exploit the more natural framework for inductive bias \cite{distributional-perspective} that is presented by DRL to optimise exploration and minimise the number of constraint violations. 

For a deep RL setting and with the distributional RL framework at our disposal, we are able to make the auxiliary loss used to optimise both the policy and returns (through actor-critic methods) a function of both the cost and reward distributions. These will be parameterised as quantile functions as per \cite{quantile-networks}; the added benefit this provides in terms of regularisation and improved training stability alone is reason for investigation. In addition, through estimations of epistemic and aleatoric uncertainties of both distributions \cite{risk-uncertainty}, we should be able to parameterise exploration in terms of risk-adversity for safety and maximisation of returns. 

We will attempt to solve the constraint satisfaction problem using this idea in combination with both the SAUTE \cite{saute} formulation of a CMDP and also by applying constraints through the Lagrangian multiplier method, and compare the two. 

\chapter{Evaluation}

\section{Distributional Constrained Policy Optimization}

\subsection{The Effect of $N$}

\subsection{The Effect of $\beta$}

\subsection{Ablation of Cost Reshaping}

\section{Performance Against State-of-the-Art Algorithms}

\chapter{Conclusion}

\section{Future Work}

%% bibliography
\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}

@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}

@article{SilverHuangEtAl16nature,
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8 percent winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  added-at = {2016-05-21T09:09:48.000+0200},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/29e987f58d895c490144693139cbc90c7/flint63},
  doi = {10.1038/nature16961},
  file = {Nature online:2016/SilverHuangEtAl16nature.pdf:PDF},
  groups = {public},
  interhash = {48430c7891aaf9fe2582faa8f5d076c1},
  intrahash = {9e987f58d895c490144693139cbc90c7},
  issn = {0028-0836},
  journal = {Nature},
  keywords = {01614 paper ai google learn algorithm},
  month = {#jan#},
  number = 7587,
  pages = {484--489},
  timestamp = {2018-04-16T12:03:12.000+0200},
  title = {Mastering the Game of {Go} with Deep Neural Networks and Tree Search},
  username = {flint63},
  volume = 529,
  year = 2016
}

 @article{bellman_1958, title={Dynamic programming and Stochastic Control Processes}, volume={1}, DOI={10.1016/s0019-9958(58)80003-0}, number={3}, journal={Information and Control}, author={Bellman, Richard}, year={1958}, pages={228–239}} 
 
 @article{puterman_1994, title={Markov decision processes}, DOI={10.1002/9780470316887}, journal={Wiley Series in Probability and Statistics}, author={Puterman, Martin L.}, year={1994}} 
 
 @TECHREPORT{sarsa,
    author = {G. A. Rummery and M. Niranjan},
    title = {On-Line Q-Learning Using Connectionist Systems},
    institution = {},
    year = {1994}
}

 @article{q-learning, title={Q-learning}, volume={8}, DOI={10.1007/bf00992698}, number={3-4}, journal={Machine Learning}, author={Watkins, Christopher J. and Dayan, Peter}, year={1992}, pages={279–292}} 
 
 @book{bellman-dp,
  abstract = {{An introduction to the mathematical theory of multistage decision processes, this text takes a "functional equation" approach to the discovery of optimum policies. Written by a leading developer of such policies, it presents a series of methods, uniqueness and existence theorems, and examples for solving the relevant equations. The text examines existence and uniqueness theorems, the optimal inventory equation, bottleneck problems in multistage production processes, a new formalism in the calculus of variation, strategies behind multistage games, and Markovian decision processes. Each chapter concludes with a problem set that Eric V. Denardo of Yale University, in his informative new introduction, calls "a rich lode of applications and research topics." 1957 edition. 37 figures.}},
  added-at = {2011-08-17T16:08:47.000+0200},
  author = {Bellman, Richard},
  biburl = {https://www.bibsonomy.org/bibtex/29cdd821222218ded252c8ba5cd712666/pcbouman},
  interhash = {acf948462171ca060064a7ded257a792},
  intrahash = {9cdd821222218ded252c8ba5cd712666},
  isbn = {9780486428093},
  keywords = {book dynamic programming},
  publisher = {Dover Publications},
  timestamp = {2011-08-18T09:10:27.000+0200},
  title = {{Dynamic Programming}},
  year = 1957
}

@misc{deep-rl,
  doi = {10.48550/ARXIV.1312.5602},
  url = {https://arxiv.org/abs/1312.5602},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Playing Atari with Deep Reinforcement Learning},
  publisher = {arXiv},
  year = {2013},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

 @article{deep-q-networks, 
 title={Human-level control through deep reinforcement learning}, volume={518}, DOI={10.1038/nature14236}, number={7540}, journal={Nature}, author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and et al.}, year={2015}, pages={529–533}} 

@inproceedings{experience-replay,
  title={Reinforcement learning for robots using neural networks},
  author={Longxin Lin},
  year={1992}
}

@misc{DDQN,
  doi = {10.48550/ARXIV.1509.06461},
  
  url = {https://arxiv.org/abs/1509.06461},
  
  author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Deep Reinforcement Learning with Double Q-learning},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

\textbf{@inproceedings{policy-gradients,
author = {Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
year = {1999},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
booktitle = {Proceedings of the 12th International Conference on Neural Information Processing Systems},
pages = {1057–1063},
numpages = {7},
location = {Denver, CO},
series = {NIPS'99}
}
}


@misc{DDPG,
  doi = {10.48550/ARXIV.1509.02971},
  
  url = {https://arxiv.org/abs/1509.02971},
  
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Continuous control with deep reinforcement learning},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@INPROCEEDINGS{actor-critic-algorithms,
    author = {Vijay Konda and John Tsitsiklis},
    title = {Actor-Critic Algorithms},
    booktitle = {SIAM Journal on Control and Optimization},
    year = {2000},
    pages = {1008--1014},
    publisher = {MIT Press}
}

@article{PG-algorithms,
  title   = "Policy Gradient Algorithms",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2018",
  url     = "https://lilianweng.github.io/posts/2018-04-08-policy-gradient/"
}

@misc{SAC,
  doi = {10.48550/ARXIV.1801.01290},
  
  url = {https://arxiv.org/abs/1801.01290},
  
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{trpo,
  doi = {10.48550/ARXIV.1502.05477},
  
  url = {https://arxiv.org/abs/1502.05477},
  
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Trust Region Policy Optimization},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{distributional-perspective,
  doi = {10.48550/ARXIV.1707.06887},
  
  url = {https://arxiv.org/abs/1707.06887},
  
  author = {Bellemare, Marc G. and Dabney, Will and Munos, Rémi},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Distributional Perspective on Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{quantile-regression,
  doi = {10.48550/ARXIV.1710.10044},
  
  url = {https://arxiv.org/abs/1710.10044},
  
  author = {Dabney, Will and Rowland, Mark and Bellemare, Marc G. and Munos, Rémi},
  
  keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Distributional Reinforcement Learning with Quantile Regression},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{quantile-networks,
  doi = {10.48550/ARXIV.1806.06923},
  
  url = {https://arxiv.org/abs/1806.06923},
  
  author = {Dabney, Will and Ostrovski, Georg and Silver, David and Munos, Rémi},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Implicit Quantile Networks for Distributional Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{MCMC,
title = "A simple introduction to Markov Chain Monte-Carlo sampling",
abstract = "Markov Chain Monte–Carlo (MCMC) is an increasingly popular method for obtaining information about distributions, especially for estimating posterior distributions in Bayesian inference. This article provides a very basic introduction to MCMC sampling. It describes what MCMC is, and what it can be used for, with simple illustrative examples. Highlighted are some of the benefits and limitations of MCMC sampling, as well as different approaches to circumventing the limitations most likely to trouble cognitive scientists.",
keywords = "MEMORY, CHOICE, GIBBS SAMPLER, MODEL",
author = "{van Ravenzwaaij}, Don and Pete Cassey and Brown, {Scott D.}",
year = "2018",
month = feb,
doi = "10.3758/s13423-016-1015-8",
language = "English",
volume = "25",
pages = "143--154",
journal = "Psychonomic Bulletin & Review",
issn = "1069-9384",
publisher = "SPRINGER",
number = "1",
}

@misc{risk-uncertainty,
  doi = {10.48550/ARXIV.1905.09638},
  
  url = {https://arxiv.org/abs/1905.09638},
  
  author = {Clements, William R. and Van Delft, Bastien and Robaglia, Benoît-Marie and Slaoui, Reda Bahi and Toth, Sébastien},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Estimating Risk and Uncertainty in Deep Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@InProceedings{map-sampling,
  title = 	 {Uncertainty in Neural Networks: Approximately Bayesian Ensembling},
  author =       {Pearce, Tim and Leibfried, Felix and Brintrup, Alexandra},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {234--244},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/pearce20a/pearce20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/pearce20a.html},
  abstract = 	 {Understanding the uncertainty of a neural network’s (NN) predictions is essential for many purposes. The Bayesian framework provides a principled approach to this, however applying it to NNs is challenging due to large numbers of parameters and data. Ensembling NNs provides an easily implementable, scalable method for uncertainty quantification, however, it has been criticised for not being Bayesian. This work proposes one modification to the usual process that we argue does result in approximate Bayesian inference; regularising parameters about values drawn from a distribution which can be set equal to the prior. A theoretical analysis of the procedure in a simplified setting suggests the recovered posterior is centred correctly but tends to have an underestimated marginal variance, and overestimated correlation. However, two conditions can lead to exact recovery. We argue that these conditions are partially present in NNs.  Empirical evaluations demonstrate it has an advantage over standard ensembling, and is competitive with variational methods.}
}

@misc{conservative-distributional-rl,
  doi = {10.48550/ARXIV.2201.07286},
  
  url = {https://arxiv.org/abs/2201.07286},
  
  author = {Zhang, Hengrui and Lin, Youfang and Han, Sheng and Wang, Shuo and Lv, Kai},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Conservative Distributional Reinforcement Learning with Safety Constraints},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{cpo,
  doi = {10.48550/ARXIV.1705.10528},
  
  url = {https://arxiv.org/abs/1705.10528},
  
  author = {Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Constrained Policy Optimization},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{saute,
  doi = {10.48550/ARXIV.2202.06558},
  
  url = {https://arxiv.org/abs/2202.06558},
  
  author = {Sootla, Aivar and Cowen-Rivers, Alexander I. and Jafferjee, Taher and Wang, Ziyan and Mguni, David and Wang, Jun and Bou-Ammar, Haitham},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {SAUTE RL: Almost Surely Safe Reinforcement Learning Using State Augmentation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

